{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import importlib\n",
    "import copy\n",
    "import argparse\n",
    "from torchvision import transforms, datasets\n",
    "import backpack\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from scipy.sparse.linalg import LinearOperator\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from torch.autograd import Variable, grad\n",
    "from numpy.linalg import eig as eig\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from utils import *\n",
    "from models.fc import *\n",
    "from models.wide_resnet import wide_resnet_t\n",
    "from models.wide_resnet_1 import WideResNet\n",
    "import scipy\n",
    "from scipy.linalg import eigh_tridiagonal\n",
    "from functions import *\n",
    "from scipy.optimize import minimize\n",
    "from dataset import *\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from backpack import extend, backpack\n",
    "from backpack.extensions import (\n",
    "    GGNMP,\n",
    "    HMP,\n",
    "    KFAC,\n",
    "    KFLR,\n",
    "    KFRA,\n",
    "    PCHMP,\n",
    "    BatchDiagGGNExact,\n",
    "    BatchDiagGGNMC,\n",
    "    BatchDiagHessian,\n",
    "    BatchGrad,\n",
    "    BatchL2Grad,\n",
    "    DiagGGNExact,\n",
    "    DiagGGNMC,\n",
    "    DiagHessian,\n",
    "    SumGradSquared,\n",
    "    Variance,\n",
    ")\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %aimport functions\n",
    "# %aimport analyze\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/exp/random_0.001/\n"
     ]
    }
   ],
   "source": [
    "## preperation\n",
    "\n",
    "num_train = 50000\n",
    "num_val = 10000\n",
    "num_d = 200\n",
    "b = 0.001\n",
    "c = 50*b\n",
    "num_nt = 50\n",
    "num_ns = 1000\n",
    "args = (10, 1, num_ns)\n",
    "model_name = \"random\"\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "path = \"/exp/random_\" + str(b) +\"/\"\n",
    "print(path)\n",
    "mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data generation\n",
    "\n",
    "data_train = torch.randn(num_train, num_d)*c*torch.exp(-b*torch.arange(num_d)) / np.sqrt(num_d)\n",
    "data_val = torch.randn(num_val, num_d)*c*torch.exp(-b*torch.arange(num_d)) / np.sqrt(num_d)\n",
    "\n",
    "print(data_train.shape, data_val.shape)\n",
    "e, v = torch.eig(data_train.T@data_train / num_train)\n",
    "e = e[:, 0]\n",
    "idx = list(np.flip(e.numpy().argsort()))\n",
    "e = e[idx]\n",
    "plt.plot(np.real(e.numpy()))\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "## targets generation\n",
    "\n",
    "w1 = torch.randn(num_d, num_nt) / torch.sqrt(torch.tensor(num_d))\n",
    "w2 = torch.randn(num_nt, 10) / torch.sqrt(torch.tensor(num_nt))\n",
    "\n",
    "def f(data, w1, w2):\n",
    "    out = torch.tanh(data@w1)@w2\n",
    "    targets = torch.argmax(out, dim=1)\n",
    "    return targets\n",
    "\n",
    "targets_train = f(data_train, w1, w2)\n",
    "targets_val = f(data_val, w1, w2)\n",
    "\n",
    "torch.save((data_train, targets_train), path + \"train_data.pt\")\n",
    "torch.save((data_val, targets_val), path + \"val_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data loader\n",
    "\n",
    "def create_loader(data, targets, bs):\n",
    "    loader = [(data[i:i+bs, :], targets[i:i+bs]) for i in range(len(data) // bs)]\n",
    "    return loader\n",
    "\n",
    "def val(model, device, val_loader, criterion):\n",
    "    sum_loss, sum_correct = 0, 0\n",
    "    model.eval()\n",
    "    ns = 0\n",
    "    # with torch.no_grad():\n",
    "    for data, target in val_loader:\n",
    "        ns += len(data)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "\n",
    "        pred = output.max(1)[1]\n",
    "        sum_correct += pred.eq(target).sum().item()\n",
    "        sum_loss += len(data) * criterion(output, target).item()\n",
    "\n",
    "    # print(ns)\n",
    "\n",
    "    return 1 - (sum_correct / ns), sum_loss / ns\n",
    "\n",
    "\n",
    "\n",
    "data_train, targets_train = torch.load(path + \"train_data.pt\")\n",
    "data_val, targets_val = torch.load(path + \"val_data.pt\")\n",
    "\n",
    "train_loader = create_loader(data_train, targets_train, 1000)\n",
    "test_loader = create_loader(data_val, targets_val, 1000)\n",
    "train_loader_FIM = create_loader(data_train[:5000], targets_train[:5000], 1)\n",
    "train_loader_prior = create_loader(data_train[:10000], targets_train[:10000], 10000)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "model = Network1(*args).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "model = Network1(*args).to(device)\n",
    "torch.save(model.state_dict(), path + \"model_init.pt\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "scheduler = CosineAnnealingLR(optimizer,T_max=epochs, eta_min = 1e-5)\n",
    "\n",
    "val_err, val_loss = val(model, device, test_loader, criterion)\n",
    "print(val_err, val_loss)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "    for data, targets in train_loader:\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        out = model(data)\n",
    "        loss = criterion(out, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    if epoch == epochs//4:\n",
    "        torch.save(model.state_dict(), path + \"model_mid.pt\")\n",
    "\n",
    "\n",
    "    train_err, train_loss = val(model, device, train_loader, criterion)\n",
    "    print(train_err, train_loss)\n",
    "\n",
    "\n",
    "val_err, val_loss = val(model, device, test_loader, criterion)\n",
    "print(val_err, val_loss)\n",
    "\n",
    "torch.save(model.state_dict(), path + \"model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.3885, device='cuda:0', grad_fn=<NormBackward1>) tensor(347.9543, device='cuda:0', grad_fn=<NormBackward1>) tensor(668.3050, device='cuda:0', grad_fn=<NormBackward1>)\n",
      "mid 0.36758 1.8297062039375305 0.6275999999999999 2.029688262939453\n",
      "end 0.07491999999999999 0.5744844317436218 0.5035000000000001 1.3796849489212035\n"
     ]
    }
   ],
   "source": [
    "## saving model statistics\n",
    "\n",
    "model = Network1(*args)\n",
    "model.load_state_dict(torch.load(path + \"model.pt\", map_location='cpu'))\n",
    "model_init = Network1(*args)\n",
    "model_init.load_state_dict(torch.load(path + \"model_init.pt\", map_location='cpu'))\n",
    "model_mid = Network1(*args)\n",
    "model_mid.load_state_dict(torch.load(path + \"model_mid.pt\", map_location='cpu'))\n",
    "model = model.to(device)\n",
    "model_init = model_init.to(device)\n",
    "model_mid = model_mid.to(device)\n",
    "norm_init = torch.norm(list_to_vec(list(model_init.parameters())), p=2)\n",
    "norm_mid = torch.norm(list_to_vec(list(model_mid.parameters())), p=2)\n",
    "norm_trained = torch.norm(list_to_vec(list(model.parameters())), p=2)\n",
    "print(norm_init, norm_mid, norm_trained)\n",
    "tr_err_mid, tr_loss_mid = val(model_mid, device, train_loader, criterion)\n",
    "val_err_mid, val_loss_mid = val(model_mid, device, test_loader, criterion)\n",
    "print(\"mid\",tr_err_mid, tr_loss_mid, val_err_mid, val_loss_mid)\n",
    "tr_err, tr_loss = val(model, device, train_loader, criterion)\n",
    "val_err, val_loss = val(model, device, test_loader, criterion)\n",
    "print(\"end\", tr_err, tr_loss, val_err, val_loss)\n",
    "\n",
    "stat_model = dict({'te': tr_err, 'tl':tr_loss, 've':val_err, 'vl':val_loss, 'tem':tr_err_mid, 'tlm':tr_loss_mid, 'vem':val_err_mid, 'vlm':val_loss_mid})\n",
    "torch.save(stat_model, path + \"stat_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "###### kfac of FIM at init and Hess at end ######\n",
    "###### and their top eigen values and vecs ######\n",
    "#################################################\n",
    "\n",
    "\n",
    "kfac_list_init = FIM_kfac(model_init, train_loader_prior, mc=500, device=\"cpu\", mode = \"kfac\", empirical = True)\n",
    "eigspace_list_init, eigval_list_init = eigspace_FIM_kron(kfac_list_init)\n",
    "\n",
    "kfac_list_end = FIM_kfac(model, train_loader_prior, mc=500, device=\"cpu\", mode = \"kfac\", empirical = True)\n",
    "eigspace_list_end, eigval_list_end = eigspace_FIM_kron(kfac_list_end)\n",
    "\n",
    "torch.save((kfac_list_init, eigspace_list_init, eigval_list_init), path + \"kfac_all_init.pt\")\n",
    "torch.save((kfac_list_end, eigspace_list_end, eigval_list_end), path + \"kfac_all_end.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.9212803e+01  3.3591747e+01  2.9878529e+01 ... -5.0256205e-07\n",
      " -5.1182661e-07 -5.2241938e-07]\n",
      "[ 3.2783978e+01  2.9483574e+01  2.5158785e+01 ... -3.3688627e-07\n",
      " -3.3746048e-07 -3.4574586e-07]\n",
      "[ 2.6105656e+01  2.3985191e+01  2.0163393e+01 ... -5.7532304e-07\n",
      " -6.4946170e-07 -7.3091178e-07]\n",
      "[ 2.0888996e+01  1.7683691e+01  1.6524782e+01 ... -4.9012101e-07\n",
      " -5.1209855e-07 -6.6103672e-07]\n",
      "[ 2.5772293e+02  4.3421935e-02  3.7069097e-02 ... -1.1970051e-06\n",
      " -1.2013277e-06 -1.2048131e-06]\n"
     ]
    }
   ],
   "source": [
    "# FIM at initialization and end of training, logit jacobian at end\n",
    "\n",
    "L_i = FIM_truex(model_init, criterion, train_loader_FIM, \"cpu\")\n",
    "print(L_i)\n",
    "L_mid = FIM_truex(model_mid, criterion, train_loader_FIM, \"cpu\")\n",
    "print(L_mid)\n",
    "L_tt = FIM_truex(model, criterion, train_loader_FIM, \"cpu\")\n",
    "print(L_tt)\n",
    "L_te = FIM2x(model, criterion, train_loader_FIM, \"cpu\")\n",
    "print(L_te)\n",
    "L_logit = logit_jacobianx(model, 0, criterion, train_loader_FIM, 'cpu')\n",
    "print(L_logit)\n",
    "torch.save(L_i, path + \"FIM_true_init.pt\")\n",
    "torch.save(L_mid, path + \"FIM_true_mid.pt\")\n",
    "torch.save(L_tt, path + \"FIM_true_end.pt\")\n",
    "torch.save(L_te, path + \"FIM_em_end.pt\")\n",
    "torch.save(L_logit, path + \"FIM_logit_end.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hessian at the end of training\n",
    "\n",
    "eig_hess, u_hess = hess_scipy(model, 500, train_loader, criterion, device)\n",
    "torch.save((eig_hess, u_hess), path + \"eig_hess_scipy.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overlaps\n",
    "\n",
    "model = Network1(*args)\n",
    "model.load_state_dict(torch.load(path + \"model.pt\"))\n",
    "model_init = Network1(*args)\n",
    "model_init.load_state_dict(torch.load(path + \"model_init.pt\"))\n",
    "model = model.to(device)\n",
    "model_init = model_init.to(device)\n",
    "\n",
    "FIM_i, L_i, u_i = torch.load(path + \"FIM_true_init.pt\")\n",
    "FIM_tt, L_tt, u_tt = torch.load(path + \"FIM_true_end.pt\")\n",
    "FIM_te, L_te, u_te = torch.load(path + \"FIM_em_end.pt\")\n",
    "\n",
    "over = overlap(u_i, u_tt, 500, device)\n",
    "diff = list_to_vec(diff_list(list(model.parameters()), list(model_init.parameters())))\n",
    "frac_all = proj(diff, u_i, 300, device)\n",
    "frac_all = frac_all.detach().cpu()\n",
    "\n",
    "stat = dict({\"data_eig\":e, \"L_i\":L_i, \"L_mid\":L_mid, \"L_tt\":L_tt, \"L_te\":L_te, \"L_hess\":eig_hess, \"L_logit\":L_logit})\n",
    "torch.save(stat, path + \"stat.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9dc663c12342db218673069099c5f4d734584a05de3b02b725edaf289dd9612"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('pytorch_latest_p37': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
